{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ニューラルネットワーク入門"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- ニューラルネットワークは、あるニューロンの出力が別のニューロンの入力になり刺激が伝搬していくようなイメージ(脳の神経モデルの話)\n",
    "- 各ニューロンの計算を複数接続することで複雑な計算をすることが出来る\n",
    "- これを数学的なモデルに落とし込んだものがパーセプトロン\n",
    "- n個の入力に対してそれぞれ対応したn個の重みと固定値の入力バイアス(b)がある"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 単純パーセプトロン"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3093841557917043\n",
      "0.8256347143825868\n"
     ]
    }
   ],
   "source": [
    "#  単純パーセプトロンのコード\n",
    "\n",
    "import math\n",
    "\n",
    "def f(z):\n",
    "    return 1.0 / (1.0 + math.exp(-z)) # Sigmoid\n",
    "\n",
    "def perceptron(x):\n",
    "    w = [-1.64, -0.98, 1.31]\n",
    "    b = -0.05\n",
    "    \n",
    "    z= b+ x[0] * w[0] + x[1] * w[1] + x[2] * w[2]\n",
    "    output = f(z)\n",
    "    \n",
    "    return output\n",
    "\n",
    "x = [0.2, 0.3, -0.1]\n",
    "print(perceptron(x))\n",
    "x= [-0.2, -0.1, 0.9]\n",
    "print(perceptron(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3093841557917043\n",
      "0.8256347143825868\n"
     ]
    }
   ],
   "source": [
    "# Numpy使って簡略化\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def f(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z)) # Sigmoid\n",
    "\n",
    "def perceptron(x):\n",
    "    w = np.array([-1.64, -0.98, 1.31])\n",
    "    b = -0.05\n",
    "    \n",
    "    # 内積をnp.dotで計算してくれる\n",
    "    z = b + np.dot(x, w)\n",
    "    output = f(z)\n",
    "    \n",
    "    return output\n",
    "\n",
    "x = np.array([0.2, 0.3, -0.1])\n",
    "print(perceptron(x))\n",
    "x= np.array([-0.2, -0.1, 0.9])\n",
    "print(perceptron(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 同じ結果が出る\n",
    "- np.dotめちゃ便利\n",
    "- sigmoidの出力は0-1の間の結果を返すので、2クラス分類のタスクにも応用出来る\n",
    "- 単純パーセプトロンを識別器とした場合は、重み(w,b)の最適値を求めることが学習になる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 多層パーセプトロン"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 単純パーセプトロンの出力を別の単純パーセプトロンの入力にすることで多数のパーセプトロンをつなげる構造をとるのが多層パーセプトロン\n",
    "- 多層パーセプトロンで同列の並びが単純パーセプトロンがレイヤ\n",
    "- レイヤの中の単純パーセプトロンのことをユニットという\n",
    "- 単純パーセプトロンが10個ならんだレイヤ= ユニット数10のレイヤ\n",
    "- 入力ベクトル　入力層\n",
    "- 最後の出力 出力層、最終層\n",
    "- 間が中間層とか隠れ層(昔たかパイと島田さんが隠れ層の話をしていたことを思い出した)\n",
    "- 最終層を1ユニットにすることで出力値をsigmoidにして2クラス識別器的に使うことが出来る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0211 1.5785]\n"
     ]
    }
   ],
   "source": [
    "# 多層パーセプトロンのコードサンプル numpy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def f_1(z):\n",
    "    return np.maximum(z, 0) #Relu\n",
    "\n",
    "def layer_1(x):\n",
    "    # それぞれの入力に対する重み\n",
    "    W_1 = np.array([\n",
    "    [-0.423, -0.795, 1.223],\n",
    "    [1.402, 0.885, -1.701],\n",
    "    ])\n",
    "    # 固定値の入力バイアス\n",
    "    b_1 = np.array([0.546, 0.774])\n",
    "    \n",
    "    z_1 = b_1 + np.dot(W_1, x)\n",
    "    output_1 = f_1(z_1)\n",
    "    \n",
    "    return output_1\n",
    "\n",
    "x = np.array([0.2, 0.4, -0.1])\n",
    "out_1 = layer_1(x)\n",
    "print(out_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09041578]\n"
     ]
    }
   ],
   "source": [
    "# 次の層\n",
    "\n",
    "def f_2(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z)) # Sigmoid\n",
    "\n",
    "def layer_2(x):\n",
    "    # それぞれの入力に対する重み\n",
    "    W_2 = np.array([\n",
    "    [1.567, -1.645],\n",
    "    ])\n",
    "    # 固定値の入力バイアス\n",
    "    b_2 = np.array([ 0.255])\n",
    "    \n",
    "    z_2 = b_2 + np.dot(W_2, x)\n",
    "    output_2 = f_2(z_2)\n",
    "    return output_2\n",
    "\n",
    "out_2 = layer_2(out_1)\n",
    "print(out_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Karas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Karasはニューラルネットワークの操作を抽象化したライブラリ\n",
    "- バックエンドではTensorFlowがデフォルトで使われている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/shnagai/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# Karasによる多層パーセプトロンの実装\n",
    "\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Denseクラスで層を表現\n",
    "model.add(Dense(units=2, activation='relu', input_dim=3))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Karasでは角層の重みは乱数で初期化される\n",
    "  - 初期値にはいくつかのパターンがある！！\n",
    "- 具体的な重みは隠蔽、抽象化されて、層の単位でコードを書く\n",
    "- 重みにアクセスすることは可能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.558535  , 0.32071495],\n",
      "       [0.09770143, 1.0626979 ],\n",
      "       [0.57016635, 0.02764893]], dtype=float32), array([0., 0.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# 重みにアクセスする\n",
    "print(model.layers[0].get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 活性化関数(activation function)\n",
    "\n",
    "- activateで表現される\n",
    "- パーセプトロンでは、全ての入力に重みが掛け合わされてそれらをすべて足して、その結果に対して変換をかける(サンプロコードのf関数)\n",
    "- この変換をかける関数のことを活性化関数という\n",
    "\n",
    "メジャーな関数\n",
    "- Relu まずはこれだけ押さえればOK 精度や収束しやすさで優れた性能を備える 近年ではほとんどReluかその発展形が使われる\n",
    "- Sigmoid 2クラス識別器として最終層に使われる 出力が0-1の間に収束されるので出力値を確立のようにみなせる\n",
    "- Hyperbolic tangent(tanh) Sigmoidと同じ用途で使われるが、出力が-1-1なので範囲が広い分sigmoidより高性能 \n",
    "\n",
    "Sigmoidとtanhは最終層に使われる\n",
    "\n",
    "Reluは a<=0 のときは0\n",
    "a>0のときはその値を返すが、なぜそれがいいかはわかっていない。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 損失関数\n",
    "\n",
    "- loss引数で指定するもの\n",
    "- 識別器の出力と学習データがどのような意味で近いことが望ましいかを定義するのが損失関数\n",
    "- 損失関数が定義する望ましさに向かって学習するので重要だが、パターンがある程度決まっており解いている問題次第で自ずと決まってくるもの"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 最適化手法(optimizer)\n",
    "- 多層パーセプトロンは損失関数によって定義された望ましさに向かってパラメータを調整していく、これがパラメータの最適化\n",
    "- その最適化の手法をoptimizerで定義する\n",
    "\n",
    "Adamを使う\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 学習 エポックとバッチサイズ\n",
    "- fit関数にはepochsとbatch_sizeというパラーメータがある\n",
    "- 学習時に繰り返し処理で徐々に重みを変えることでじわじわと最適化\n",
    "- この学習時のパラメータがエポックとバッチサイズ\n",
    "\n",
    "- この流れを繰り返す\n",
    "　　- 1. xのうちbatch_size個をパーセプトロンに入力し、最終出力(予測結果)を得る\n",
    "  - 2. 予測結果と学習データを比較し、損失関数によって損失を求める\n",
    "  - 3. 損失を小さくするように重みを更新\n",
    "  \n",
    " - 全ての学習データを全件使い切ると1エポック\n",
    " - バッチサイズは1回の処理でどれくらいのデータを使うかを定義\n",
    " \n",
    " エポック128\n",
    " バッチも1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 学習率\n",
    "- 繰り返しでじわじわと重みを最適化していくが、一回の重みの更新でどの程度の重みの値を増減させるか決めるパラメータが学習率\n",
    "- 大きいと1回あたりの変化大きいという意味\n",
    "- ベストプラクティスは大きな学習率で大雑把に重みを最適値に近づけてその後学習率を小さくして最適値を目指す\n",
    "\n",
    "0.05,0.01とかがデフォルト値のことが多い\n",
    "\n",
    "- 重みを流用するってイメージなのかな??あんまし具体的にイメージ出来てない\n",
    "\n",
    "逆伝搬するときの変化率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ニューラルネットワークによる識別器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as numpy\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=100, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "mode.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "X = np.array([\n",
    "    []\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- one-hot encoding出てきた　これはsoftmaxとイコールかな\n",
    "- 最終層ユニット数が2以上の多層パーセプトロンを使った多クラス識別にはsoftmaxを使う。こんな特徴がある。\n",
    "  - 出力が0と1の間に収まるので、その次元が1でそれ以外は0という設定にマッチ(one-hot encoding)\n",
    "  - 活性化関数を適用した層のすべての出力の和が1 (単純に1 or 0で出るって話??ちょっとよくわからないので聞く)\n",
    "  - Softmaxを適用した層の各ユニットの出力値の、大きい値と小さい値の差が開く(Softmaxを通すと大小の比が大きくなる)\n",
    "- 明確に0か1ではなく、0か1に使い値という扱いになるから、出力が最も大きい場合にそのクラスみたいな判定として使う\n",
    "- 「i番目のユニットの出力が1でそれ以外が0なら、クラスID=iと予測する」場合には、損失関数にcategorical_crossentropyを使う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/shnagai/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/shnagai/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/shnagai/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/shnagai/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/shnagai/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/shnagai/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# kerasでone-hot表現に変換　便利ツール\n",
    "\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "y = np.array([0,1,2,3,4])\n",
    "y_one_hot = to_categorical(y)\n",
    "\n",
    "print(y_one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- sparse_categorical_crossentropyが、one-hotのパーセプトロンの出力と非one-hot表現の教師データの橋渡しをするってのがよくわかってないぞ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 09.2対話エージェントへの適用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "from os.path import dirname, join, normpath\n",
    "\n",
    "import Mecab\n",
    "import neologdn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "Class DialogueAgent:\n",
    "    def "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
