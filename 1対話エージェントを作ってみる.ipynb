{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1 対話型エージェント"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "私\n",
      "は\n",
      "わたし\n",
      "の\n",
      "こと\n",
      "名詞,代名詞,一般,*,*,*,私,ワタシ,ワタシ\n"
     ]
    }
   ],
   "source": [
    "import MeCab\n",
    "\n",
    "tagger = MeCab.Tagger()\n",
    "node =tagger.parseToNode('私はわたしのことが好きなあなたが好きです')\n",
    "\n",
    "# surfaceには単語が\n",
    "print(node.surface)\n",
    "print(node.next.surface)\n",
    "print(node.next.next.surface)\n",
    "print(node.next.next.next.surface)\n",
    "print(node.next.next.next.next.surface)\n",
    "print(node.next.next.next.next.next.surface)\n",
    "# featureには各情報が出てくる\n",
    "print(node.next.feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "私\n",
      "は\n",
      "わたし\n",
      "の\n",
      "こと\n",
      "が\n",
      "好き\n",
      "な\n",
      "あなた\n",
      "が\n",
      "好き\n",
      "です\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import MeCab\n",
    "\n",
    "tagger = MeCab.Tagger()\n",
    "node =tagger.parseToNode('私はわたしのことが好きなあなたが好きです')\n",
    "\n",
    "# 全部取り出す\n",
    "while node:\n",
    "    print(node.surface)\n",
    "    node = node.next\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "\n",
    "tagger = MeCab.Tagger()\n",
    "\n",
    "def tokenize(text):\n",
    "    node = tagger.parseToNode(text)\n",
    "    \n",
    "    tokens = []\n",
    "    while node:\n",
    "        if node.surface != '':\n",
    "            tokens.append(node.surface)\n",
    "            \n",
    "        node = node.next\n",
    "            \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['皆さん', 'ご存知', 'だ', 'と', 'は', '思い', 'ます', 'が', '、', '私', 'が', '野澤', 'です']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokenize('皆さんご存知だとは思いますが、私が野澤です')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "皆さん\t名詞,一般,*,*,*,*,皆さん,ミナサン,ミナサン\n",
      "ご存知\t名詞,一般,*,*,*,*,ご存知,ゴゾンジ,ゴゾンジ\n",
      "だ\t助動詞,*,*,*,特殊・ダ,基本形,だ,ダ,ダ\n",
      "と\t助詞,格助詞,引用,*,*,*,と,ト,ト\n",
      "は\t助詞,係助詞,*,*,*,*,は,ハ,ワ\n",
      "思い\t動詞,自立,*,*,五段・ワ行促音便,連用形,思う,オモイ,オモイ\n",
      "ます\t助動詞,*,*,*,特殊・マス,基本形,ます,マス,マス\n",
      "が\t助詞,接続助詞,*,*,*,*,が,ガ,ガ\n",
      "、\t記号,読点,*,*,*,*,、,、,、\n",
      "私\t名詞,代名詞,一般,*,*,*,私,ワタシ,ワタシ\n",
      "が\t助詞,格助詞,一般,*,*,*,が,ガ,ガ\n",
      "野澤\t名詞,固有名詞,人名,姓,*,*,野澤,ノザワ,ノザワ\n",
      "です\t助動詞,*,*,*,特殊・デス,基本形,です,デス,デス\n",
      "EOS\n",
      "\n",
      "皆さん ご存知 だ と は 思い ます が 、 私 が 野澤 です \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import MeCab\n",
    "\n",
    "tagger2 = MeCab.Tagger('-Owakati')\n",
    "\n",
    "print(tagger.parse('皆さんご存知だとは思いますが、私が野澤です'))\n",
    "print(tagger2.parse('皆さんご存知だとは思いますが、私が野澤です'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- -0wakatiでスペース分割した結果だけ返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stripで空白を消す\n",
    "# splitで区切り文字分割とリスト化\n",
    "def tokenize2(text):\n",
    "    return tagger2.parse(text).strip().split(' ')\n",
    "    #return tagger2.parse(text).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['皆さん', 'ご存知', 'だ', 'と', 'は', '思い', 'ます', 'が', '、', '私', 'が', '野澤', 'です']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokenize2('皆さんご存知だとは思いますが、私が野澤です')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ベクトル化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_bow(tokenized_texts):\n",
    "    \n",
    "    vocabulary = {}\n",
    "    print(tokenized_texts)\n",
    "    # 二次元配列を一次元に\n",
    "    for tokenized_text in tokenized_texts:\n",
    "        print(tokenized_text)\n",
    "        # 配列から要素の取り出し\n",
    "        for token in tokenized_text:\n",
    "            print(token)\n",
    "            # すでにその要素が辞書にあるかをチェック\n",
    "            if token not in vocabulary:\n",
    "                # 辞書名[key] = value\n",
    "                vocabulary[token] = len(vocabulary)\n",
    "                print(vocabulary)\n",
    "    # 辞書の長さ格納\n",
    "    n_vocab = len(vocabulary)\n",
    "    \n",
    "    # range(0, 要素数)\n",
    "    # 要素数分の0の配列を作成\n",
    "    bow = [[0] * n_vocab for i in range(len(tokenized_texts))]\n",
    "    print(bow)\n",
    "    # enumerate インデックス番号 要素の潤で取得出来る\n",
    "    # iは文書自体のインデックス\n",
    "    for i, tokenized_text in enumerate(tokenized_texts):\n",
    "        print(i, tokenized_text)\n",
    "        for token in tokenized_text:\n",
    "            print(token)\n",
    "            # ワードのindex番号を取得\n",
    "            index = vocabulary[token]\n",
    "            print(index)\n",
    "            # その文書のワードに対して、1インクリメントする\n",
    "            bow[i][index] += 1\n",
    "    # 辞書とBag of Wordsを返す\n",
    "    return vocabulary, bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['今日', 'は', 'いい', '天気', '今日', 'は', 'いい', '雨', '今日', 'は', 'ご飯', 'を', '食べる']]\n",
      "['今日', 'は', 'いい', '天気', '今日', 'は', 'いい', '雨', '今日', 'は', 'ご飯', 'を', '食べる']\n",
      "今日\n",
      "{'今日': 0}\n",
      "は\n",
      "{'今日': 0, 'は': 1}\n",
      "いい\n",
      "{'今日': 0, 'は': 1, 'いい': 2}\n",
      "天気\n",
      "{'今日': 0, 'は': 1, 'いい': 2, '天気': 3}\n",
      "今日\n",
      "は\n",
      "いい\n",
      "雨\n",
      "{'今日': 0, 'は': 1, 'いい': 2, '天気': 3, '雨': 4}\n",
      "今日\n",
      "は\n",
      "ご飯\n",
      "{'今日': 0, 'は': 1, 'いい': 2, '天気': 3, '雨': 4, 'ご飯': 5}\n",
      "を\n",
      "{'今日': 0, 'は': 1, 'いい': 2, '天気': 3, '雨': 4, 'ご飯': 5, 'を': 6}\n",
      "食べる\n",
      "{'今日': 0, 'は': 1, 'いい': 2, '天気': 3, '雨': 4, 'ご飯': 5, 'を': 6, '食べる': 7}\n",
      "[[0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "0 ['今日', 'は', 'いい', '天気', '今日', 'は', 'いい', '雨', '今日', 'は', 'ご飯', 'を', '食べる']\n",
      "今日\n",
      "0\n",
      "は\n",
      "1\n",
      "いい\n",
      "2\n",
      "天気\n",
      "3\n",
      "今日\n",
      "0\n",
      "は\n",
      "1\n",
      "いい\n",
      "2\n",
      "雨\n",
      "4\n",
      "今日\n",
      "0\n",
      "は\n",
      "1\n",
      "ご飯\n",
      "5\n",
      "を\n",
      "6\n",
      "食べる\n",
      "7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'今日': 0, 'は': 1, 'いい': 2, '天気': 3, '雨': 4, 'ご飯': 5, 'を': 6, '食べる': 7},\n",
       " [[3, 3, 2, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_texts = [tokenize('今日はいい天気今日はいい雨今日はご飯を食べる')]\n",
    "calc_bow(tokenized_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
